from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from contextlib import asynccontextmanager
import httpx
import uvicorn
import asyncio
import json
import os
import re
import time as time_module
from datetime import datetime, timedelta, time, timezone
from dotenv import load_dotenv

load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '.env'))

# Silence prints from this module by default. Set ENABLE_EARNINGS_LOGS=1 in env to enable.
try:
    ENABLE_EARNINGS_LOGS = os.getenv("ENABLE_EARNINGS_LOGS", "0") == "1"
except Exception:
    ENABLE_EARNINGS_LOGS = False

if not ENABLE_EARNINGS_LOGS:
    def _noop_print(*args, **kwargs):
        return None
    print = _noop_print

# ================= CONFIG =================
TRADINGVIEW_SCAN_URL = os.getenv("TRADINGVIEW_SCAN_URL") or "https://scanner.tradingview.com/{market}/scan?label-product=screener-stock-old"
# ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° DR_LIST_URL ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ DR
DR_LIST_URL = "https://api.ideatrade1.com/caldr"
# ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡∏¥‡∏î-‡∏õ‡∏¥‡∏î‡∏ü‡∏¥‡∏•‡πÄ‡∏ï‡∏≠‡∏£‡πå DR (True = ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ DR, False = ‡πÄ‡∏≠‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
ENABLE_DR_FILTER = os.getenv("ENABLE_DR_FILTER", "True").lower() == "true"
CACHE_FILE = "earnings_cache.json"
UPDATE_INTERVAL_SECONDS = int(os.getenv("EARNINGS_UPDATE_INTERVAL_SECONDS") or "3600")
# Show detailed filter warnings (set env SHOW_FILTER_WARNINGS=true to enable)
SHOW_FILTER_WARNINGS = os.getenv("SHOW_FILTER_WARNINGS", "False").lower() == "true"

FAKE_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
    "Origin": "https://www.tradingview.com",
    "Referer": "https://www.tradingview.com/"
}

COLUMNS_MAP = [
    "logoid", "name", "market_cap_basic", "earnings_per_share_forecast_next_fq",
    "earnings_per_share_fq", "eps_surprise_fq", "eps_surprise_percent_fq",
    "revenue_forecast_next_fq", "revenue_fq", "earnings_release_next_date",
    "earnings_release_next_calendar_date", "earnings_release_next_time",
    "description", "type", "subtype", "update_mode",
    "earnings_per_share_forecast_fq", "revenue_forecast_fq", "earnings_release_date",
    "earnings_release_calendar_date", "earnings_release_time", "currency",
    "fundamental_currency_code", "exchange"
]

MARKET_DISPLAY_NAMES = {
    "america": "US United States", "thailand": "TH Thailand", "hongkong": "HK Hong Kong",
    "japan": "JP Japan", "china": "CN China", "singapore": "SG Singapore",
    "vietnam": "VN Vietnam", "france": "FR France", "germany": "DE Germany",
    "netherlands": "NL Netherlands", "denmark": "DK Denmark", "italy": "IT Italy",
    "taiwan": "TW Taiwan"
}

# Global DB
_earnings_db = {}
_last_update_str = "-"
_previous_earnings_db = {}  # Store previous earnings state for comparison

# SSE Client Management
_sse_clients: list[asyncio.Queue] = []
_sse_lock = asyncio.Lock()

# ================= HELPERS =================
def get_market_code(country_code: str):
    mapping = {
        "US": "america", "TH": "thailand", "HK": "hongkong", "JP": "japan",
        "CN": "china", "SG": "singapore", "VN": "vietnam", "FR": "france",
        "DE": "germany", "NL": "netherlands", "DK": "denmark", "IT": "italy",
        "TW": "taiwan"
    }
    return mapping.get(country_code.upper())

def get_tradingview_range(country: str = "US"):
    now_utc = datetime.now(timezone.utc)
    today_date = now_utc.date()
    # ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡∏±‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏´‡∏ô‡πâ‡∏≤ (next Monday)
    # weekday() ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ 0=Monday, 1=Tuesday, ..., 6=Sunday
    # ‡∏ñ‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå (weekday=0) ‡∏à‡∏∞‡πÑ‡∏î‡πâ next_monday = today_date + 7 days
    # ‡∏ñ‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏≠‡∏±‡∏á‡∏Ñ‡∏≤‡∏£-‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå ‡∏à‡∏∞‡πÑ‡∏î‡πâ next_monday = today_date + (7 - weekday) days
    days_until_monday = 7 - today_date.weekday()
    if days_until_monday == 0:  # ‡∏ñ‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå ‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏´‡∏ô‡πâ‡∏≤
        days_until_monday = 7
    next_monday = today_date + timedelta(days=days_until_monday)
    next_monday_dt = datetime.combine(next_monday, time.min).replace(tzinfo=timezone.utc)
    offset_hours = 15 if country.upper() == "JP" else 0
    start_dt = next_monday_dt + timedelta(hours=offset_hours)
    end_dt = start_dt + timedelta(days=7)
    return int(start_dt.timestamp()), int(end_dt.timestamp())

async def fetch_tradingview_earnings(market_code: str, start_ts: int, end_ts: int):
    """Fetch earnings data from TradingView with pagination to get all results"""
    url = TRADINGVIEW_SCAN_URL.format(market=market_code)
    all_data = []
    page_size = 500  # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏•‡∏∞ 500 ‡∏ï‡∏±‡∏ß
    offset = 0
    
    async with httpx.AsyncClient() as client:
        while True:
            payload = {
                "filter": [
                    {"left": "is_primary", "operation": "equal", "right": True},
                    {"left": "earnings_release_date,earnings_release_next_date", "operation": "in_range", "right": [start_ts, end_ts]},
                    {"left": "earnings_release_date,earnings_release_next_date", "operation": "nequal", "right": end_ts}
                ],
                "options": {"lang": "th"},
                "markets": [market_code],
                "symbols": {
                    "query": {"types": []},
                    "tickers": []
                },
                "columns": COLUMNS_MAP,
                "sort": {"sortBy": "market_cap_basic", "sortOrder": "desc"},
                "preset": None,
                "range": [offset, offset + page_size]
            }
            try:
                resp = await client.post(url, json=payload, headers=FAKE_HEADERS, timeout=15)
                if resp.status_code != 200:
                    break
                
                page_data = resp.json().get("data", [])
                if not page_data:
                    break
                
                all_data.extend(page_data)
                
                # ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ page_size ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß
                if len(page_data) < page_size:
                    break
                
                offset += page_size
                
                # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà 5000 ‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô infinite loop
                if offset >= 5000:
                    break
                    
            except Exception as e:
                print(f"[WARN] Error fetching page {offset}: {e}")
                break
    
    return all_data

# ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö valid_tickers ‡πÅ‡∏•‡∏∞ ticker_mapping ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ DR
def map_tv_data_to_object(raw_data, valid_tickers: set = None, ticker_mapping: dict = None):
    mapped_list = []
    seen = set()
    current_ts = datetime.now(timezone.utc).timestamp()
    for item in raw_data:
        d = item.get("d", [])
        if not d or len(d) < len(COLUMNS_MAP): continue
        obj = {COLUMNS_MAP[i]: d[i] for i in range(len(COLUMNS_MAP))}
        
        # ‚úÖ ‡πÉ‡∏ä‡πâ logoid ‡πÄ‡∏õ‡πá‡∏ô ticker symbol (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ name ‡πÄ‡∏õ‡πá‡∏ô fallback
        # logoid ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ticker symbol ‡πÄ‡∏ä‡πà‡∏ô "AAPL", "AMZN" 
        # name ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏° ‡πÄ‡∏ä‡πà‡∏ô "APPLE INC", "AMAZON.COM, INC"
        logoid = str(obj.get("logoid") or "").upper().strip()
        ticker_name = str(obj["name"]).upper().strip()
        description = str(obj.get("description") or "").upper().strip()
        
        # ‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö matched_underlying (‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠ DR Filter ‡∏õ‡∏¥‡∏î)
        matched_underlying = None
        
        # ‚úÖ ‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Whitelist ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á logoid ‡πÅ‡∏•‡∏∞ ticker_name
        # ‡πÉ‡∏ä‡πâ ticker_mapping ‡πÄ‡∏û‡∏∑‡πà‡∏≠ match ticker ‡∏à‡∏≤‡∏Å TradingView ‡∏Å‡∏±‡∏ö underlying code
        if valid_tickers is not None:
            matched = False
            matched_ticker = None
            matched_underlying = None
            
            # ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:
            # 1) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ticker_name ‡πÉ‡∏ô valid_tickers ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            if ticker_name in valid_tickers:
                matched = True
                matched_ticker = ticker_name
                matched_underlying = ticker_name
            # 2) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ticker_name ‡πÉ‡∏ô ticker_mapping (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            elif ticker_mapping and ticker_name in ticker_mapping:
                matched_underlying = ticker_mapping[ticker_name]
                if matched_underlying in valid_tickers:
                    matched = True
                    matched_ticker = ticker_name
                    # Debug: ‡πÅ‡∏™‡∏î‡∏á warning ‡∏ñ‡πâ‡∏≤ match ‡∏Å‡∏±‡∏ö underlying code ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö ticker_name
                    if matched_underlying != ticker_name and len(mapped_list) < 10:
                        print(f"  [WARN] Warning: ticker_name='{ticker_name}' matched with underlying='{matched_underlying}' (via mapping)")
            # 3) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logoid ‡πÉ‡∏ô valid_tickers
            elif logoid and logoid in valid_tickers:
                matched = True
                matched_ticker = logoid
                matched_underlying = logoid
            # 4) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logoid ‡πÉ‡∏ô ticker_mapping (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            elif ticker_mapping and logoid and logoid in ticker_mapping:
                matched_underlying = ticker_mapping[logoid]
                if matched_underlying in valid_tickers:
                    matched = True
                    matched_ticker = logoid
            # 5) ‡∏•‡∏≠‡∏á extract ticker symbol ‡∏à‡∏≤‡∏Å name (‡∏ñ‡πâ‡∏≤ format ‡πÄ‡∏õ‡πá‡∏ô "TICKER" ‡∏´‡∏£‡∏∑‡∏≠ "TICKER - ...")
            # 6) ‡∏•‡∏≠‡∏á extract ticker symbol ‡∏à‡∏≤‡∏Å description (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            if not matched:
                # ‡∏•‡∏≠‡∏á extract ticker symbol ‡∏à‡∏≤‡∏Å name (‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô "-" ‡∏´‡∏£‡∏∑‡∏≠ ",")
                name_parts = re.split(r'[-,\s]+', ticker_name)
                for part in name_parts:
                    if part and part in valid_tickers:
                        matched = True
                        matched_ticker = part
                        matched_underlying = part
                        break
                    elif ticker_mapping and part and part in ticker_mapping:
                        matched_underlying = ticker_mapping[part]
                        if matched_underlying in valid_tickers:
                            matched = True
                            matched_ticker = part
                            break
                
                # ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ description ‡πÄ‡∏û‡∏∑‡πà‡∏≠ match ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏≤‡∏à‡∏à‡∏∞ match ‡∏ú‡∏¥‡∏î (‡πÄ‡∏ä‡πà‡∏ô "JP Morgan" -> "JP" ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à match ‡∏Å‡∏±‡∏ö underlying code ‡∏≠‡∏∑‡πà‡∏ô)
                # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ description ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏°‡∏≤‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏≤‡∏à match ‡∏ú‡∏¥‡∏î‡πÑ‡∏î‡πâ
            
            if not matched:
                # Debug: ‡πÅ‡∏™‡∏î‡∏á ticker ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà match (‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 10 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ log ‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)
                if SHOW_FILTER_WARNINGS:
                    if len([x for x in mapped_list if not hasattr(x, '_debug_shown')]) < 10:
                        print(f"  [WARN] Filtered out: logoid='{logoid}', name='{ticker_name}' (not in {len(valid_tickers)} DR tickers)")
                continue  # ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô whitelist ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
            else:
                # Debug: ‡πÅ‡∏™‡∏î‡∏á ticker ‡∏ó‡∏µ‡πà match ‡πÑ‡∏î‡πâ (‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 10 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
                if len(mapped_list) < 10:
                    print(f"  [OK] Matched: logoid='{logoid}', name='{ticker_name}' -> underlying='{matched_underlying}'")
        
        event_date = obj["earnings_release_next_date"] or obj["earnings_release_date"]
        
        # ‚úÖ ‡πÉ‡∏ä‡πâ (ticker, date) ‡πÄ‡∏õ‡πá‡∏ô unique key ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô duplicate
        # ‡πÉ‡∏ä‡πâ matched_underlying ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö DR underlying code)
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ matched_underlying ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ticker_name (‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ filter)
        final_ticker_for_key = matched_underlying if matched_underlying else ticker_name
        
        if event_date:
            unique_key = (final_ticker_for_key, event_date)
            if unique_key in seen: 
                continue
            seen.add(unique_key)
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ date ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ logoid ‡∏´‡∏£‡∏∑‡∏≠ ticker ‡πÄ‡∏õ‡πá‡∏ô fallback
            unique_id = obj.get("logoid") or final_ticker_for_key
            if unique_id in seen: 
                continue
            seen.add(unique_id)
        
        is_future = event_date and event_date > current_ts
        
        # ‚úÖ ‡πÉ‡∏ä‡πâ matched_underlying ‡πÄ‡∏õ‡πá‡∏ô ticker ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö DR underlying code)
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ matched_underlying ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ticker_name (‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ filter)
        final_ticker = matched_underlying if matched_underlying else ticker_name
        
        mapped_list.append({
            "ticker": final_ticker,
            "company": obj["description"],
            "marketCap": obj["market_cap_basic"], 
            "epsEstimate": obj["earnings_per_share_forecast_next_fq"],
            "epsReported": None if is_future else obj["earnings_per_share_fq"],
            "surprise": None if is_future else obj["eps_surprise_fq"],
            "pctSurprise": None if is_future else obj["eps_surprise_percent_fq"],
            "revenueForecast": obj["revenue_forecast_next_fq"],
            "revenueActual": None if is_future else obj["revenue_fq"],
            "date": event_date, 
            "period": obj["earnings_release_next_calendar_date"],
            "currency": obj["currency"],
            "exchange": obj["exchange"]
        })
    return mapped_list

# ================= PERSISTENCE LOGIC =================
def load_db_from_disk():
    global _earnings_db, _last_update_str, _previous_earnings_db
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                loaded = json.load(f)
                _earnings_db = loaded.get("data", {})
                _last_update_str = loaded.get("meta", {}).get("updated_at", "-")
                # Initialize previous_earnings_db with current data (so first update won't trigger false positives)
                _previous_earnings_db = _earnings_db.copy()
            print(f"[OK] Loaded cache: {len(_earnings_db)} markets.")
        except Exception as e: print(f"[WARN] Load fail: {e}")

def save_db_to_disk():
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump({"meta": {"updated_at": _last_update_str}, "data": _earnings_db}, f, ensure_ascii=False)
    except Exception as e: print(f"[WARN] Save fail: {e}")

async def broadcast_to_sse_clients(message: dict):
    """Broadcast message to all connected SSE clients"""
    async with _sse_lock:
        client_count = len(_sse_clients)
        if client_count > 0:
            print(f"[SSE] Broadcasting to {client_count} client(s): {message.get('type', 'unknown')}")
        disconnected_clients = []
        for queue in _sse_clients:
            try:
                await queue.put(message)
            except Exception as e:
                # Mark for removal if queue is closed
                print(f"[SSE] Failed to send to client: {e}")
                disconnected_clients.append(queue)
        
        # Remove disconnected clients
        for queue in disconnected_clients:
            if queue in _sse_clients:
                _sse_clients.remove(queue)

def get_earnings_set(earnings_db: dict) -> set:
    """Convert earnings_db to a set of (ticker, date) tuples for comparison"""
    earnings_set = set()
    for market_data in earnings_db.values():
        if isinstance(market_data, dict) and "data" in market_data:
            for earning in market_data["data"]:
                ticker = earning.get("ticker", "")
                date = earning.get("date")
                if ticker and date:
                    earnings_set.add((ticker, date))
    return earnings_set

def find_new_earnings(current_db: dict, previous_db: dict) -> list:
    """Find new earnings by comparing current_db with previous_db"""
    current_set = get_earnings_set(current_db)
    previous_set = get_earnings_set(previous_db)
    
    new_keys = current_set - previous_set
    
    # Extract full earning objects for new keys
    new_earnings = []
    for market_data in current_db.values():
        if isinstance(market_data, dict) and "data" in market_data:
            for earning in market_data["data"]:
                ticker = earning.get("ticker", "")
                date = earning.get("date")
                if (ticker, date) in new_keys:
                    new_earnings.append(earning)
    
    return new_earnings

async def background_updater():
    global _earnings_db, _last_update_str, _previous_earnings_db
    while True:
        try:
            print(f"[Background] Updating Earnings Data at {datetime.now()}")
            
            # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Logic ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ DR ‡∏ï‡∏≤‡∏°‡∏Ñ‡πà‡∏≤ ENABLE_DR_FILTER
            valid_dr_tickers = None
            ticker_mapping = {}  # Mapping table: {ticker_from_tv: underlying_code}
            if ENABLE_DR_FILTER:
                valid_dr_tickers = set()
                skipped_count = 0
                skipped_reasons = {}
                skipped_items = []  # ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å skip ‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
                try:
                    async with httpx.AsyncClient() as client:
                        r_dr = await client.get(DR_LIST_URL, timeout=10)
                        dr_rows = r_dr.json().get("rows", [])
                        print(f"  [Background] Total DR rows from API: {len(dr_rows)}")
                        for item in dr_rows:
                            u_code = None
                            source = None
                            
                            # ‚úÖ ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: 1) extract ‡∏à‡∏≤‡∏Å underlyingName ‡∏ó‡∏µ‡πà‡∏°‡∏µ format "(TICKER)" ‡∏Å‡πà‡∏≠‡∏ô
                            underlying_name = item.get("underlyingName") or ""
                            match = re.search(r'\(([A-Z0-9.\-_]+)\)$', underlying_name)
                            if match:
                                u_code = match.group(1)
                                source = "underlyingName"
                            else:
                                # 2) ‡πÉ‡∏ä‡πâ underlying field (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                                u_code = item.get("underlying")
                                if u_code:
                                    source = "underlying"
                                else:
                                    # 3) extract ‡∏à‡∏≤‡∏Å symbol (‡πÄ‡∏ä‡πà‡∏ô "JPM80" -> "JPM")
                                    sym = item.get("symbol") or ""
                                    if "80" in sym: 
                                        u_code = sym.replace("80", "")
                                        source = "symbol(80)"
                                    elif "19" in sym: 
                                        u_code = sym.replace("19", "")
                                        source = "symbol(19)"
                            
                            if u_code:
                                u_code = u_code.strip().upper()
                                # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ticker symbol (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ space ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß)
                                # ‡πÄ‡∏ä‡πà‡∏ô "JPM", "BAC", "CNSEMI" ‡∏ú‡πà‡∏≤‡∏ô ‡πÅ‡∏ï‡πà "CNSEMI ETF" ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô
                                # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡∏°‡∏µ dash/underscore/dot ‡πÑ‡∏î‡πâ (‡πÄ‡∏ä‡πà‡∏ô "A-B", "A_B", "A.B")
                                if u_code and len(u_code) > 0:
                                    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ space ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏° ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á extract ticker ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ
                                    if ' ' in u_code:
                                        # ‡∏ß‡∏¥‡∏ò‡∏µ 1: ‡∏•‡∏≠‡∏á extract ticker ‡∏à‡∏≤‡∏Å underlyingName ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡∏≠‡∏≤‡∏à‡∏°‡∏µ format ‡∏≠‡∏∑‡πà‡∏ô)
                                        name_match_alt = re.search(r'\(([A-Z0-9.\-_]+)\)', underlying_name.upper())
                                        if name_match_alt:
                                            alt_ticker = name_match_alt.group(1).strip()
                                            if alt_ticker and ' ' not in alt_ticker and len(alt_ticker) <= 15:
                                                u_code = alt_ticker
                                                source = "underlyingName(alt)"
                                            else:
                                                # ‡∏ß‡∏¥‡∏ò‡∏µ 2: ‡∏•‡∏≠‡∏á extract ticker ‡∏à‡∏≤‡∏Å underlying ‡πÇ‡∏î‡∏¢‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "ETF", "DIAMOND ETF" ‡∏Ø‡∏•‡∏Ø
                                                u_code_clean = re.sub(r'\s+(ETF|DIAMOND ETF|FUND|TRUST).*$', '', u_code, flags=re.IGNORECASE).strip()
                                                if u_code_clean and ' ' not in u_code_clean and len(u_code_clean) <= 15:
                                                    u_code = u_code_clean
                                                    source = "underlying(clean)"
                                                else:
                                                    # ‡∏ß‡∏¥‡∏ò‡∏µ 3: ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ symbol ‡πÇ‡∏î‡∏¢‡∏•‡∏ö suffix (‡πÄ‡∏ä‡πà‡∏ô "E1VFVN3001" -> "E1VFVN30", "FUEVFVND01" -> "FUEVFVND")
                                                    sym = item.get("symbol") or ""
                                                    if sym:
                                                        # ‡∏•‡∏ö suffix ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô "01", "24", "3001")
                                                        sym_clean = re.sub(r'\d+$', '', sym).strip()
                                                        if sym_clean and len(sym_clean) >= 2 and len(sym_clean) <= 15:
                                                            u_code = sym_clean.upper()
                                                            source = "symbol(clean)"
                                                        else:
                                                            skipped_count += 1
                                                            skipped_reasons['has_space'] = skipped_reasons.get('has_space', 0) + 1
                                                            skipped_items.append({
                                                                'symbol': item.get('symbol', 'N/A'),
                                                                'underlyingName': underlying_name[:50],
                                                                'u_code': u_code,
                                                                'reason': 'has_space'
                                                            })
                                                            continue
                                                    else:
                                                        skipped_count += 1
                                                        skipped_reasons['has_space'] = skipped_reasons.get('has_space', 0) + 1
                                                        skipped_items.append({
                                                            'symbol': item.get('symbol', 'N/A'),
                                                            'underlyingName': underlying_name[:50],
                                                            'u_code': u_code,
                                                            'reason': 'has_space'
                                                        })
                                                        continue
                                        else:
                                            # ‡∏ß‡∏¥‡∏ò‡∏µ 2: ‡∏•‡∏≠‡∏á extract ticker ‡∏à‡∏≤‡∏Å underlying ‡πÇ‡∏î‡∏¢‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "ETF", "DIAMOND ETF" ‡∏Ø‡∏•‡∏Ø
                                            u_code_clean = re.sub(r'\s+(ETF|DIAMOND ETF|FUND|TRUST).*$', '', u_code, flags=re.IGNORECASE).strip()
                                            if u_code_clean and ' ' not in u_code_clean and len(u_code_clean) <= 15:
                                                u_code = u_code_clean
                                                source = "underlying(clean)"
                                            else:
                                                # ‡∏ß‡∏¥‡∏ò‡∏µ 3: ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ symbol ‡πÇ‡∏î‡∏¢‡∏•‡∏ö suffix (‡πÄ‡∏ä‡πà‡∏ô "E1VFVN3001" -> "E1VFVN30", "FUEVFVND01" -> "FUEVFVND")
                                                sym = item.get("symbol") or ""
                                                if sym:
                                                    # ‡∏•‡∏ö suffix ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡πâ‡∏≤‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô "01", "24", "3001")
                                                    sym_clean = re.sub(r'\d+$', '', sym).strip()
                                                    if sym_clean and len(sym_clean) >= 2 and len(sym_clean) <= 15:
                                                        u_code = sym_clean.upper()
                                                        source = "symbol(clean)"
                                                    else:
                                                        skipped_count += 1
                                                        skipped_reasons['has_space'] = skipped_reasons.get('has_space', 0) + 1
                                                        skipped_items.append({
                                                            'symbol': item.get('symbol', 'N/A'),
                                                            'underlyingName': underlying_name[:50],
                                                            'u_code': u_code,
                                                            'reason': 'has_space'
                                                        })
                                                        continue
                                                else:
                                                    skipped_count += 1
                                                    skipped_reasons['has_space'] = skipped_reasons.get('has_space', 0) + 1
                                                    skipped_items.append({
                                                        'symbol': item.get('symbol', 'N/A'),
                                                        'underlyingName': underlying_name[:50],
                                                        'u_code': u_code,
                                                        'reason': 'has_space'
                                                    })
                                                    continue
                                    # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô 15 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏° ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
                                    if len(u_code) > 15:
                                        skipped_count += 1
                                        skipped_reasons['too_long'] = skipped_reasons.get('too_long', 0) + 1
                                        skipped_items.append({
                                            'symbol': item.get('symbol', 'N/A'),
                                            'underlyingName': underlying_name[:50],
                                            'u_code': u_code,
                                            'reason': 'too_long'
                                        })
                                        continue
                                    valid_dr_tickers.add(u_code)
                                    
                                    # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á mapping table: ‡πÉ‡∏ä‡πâ underlying code ‡πÄ‡∏õ‡πá‡∏ô key ‡πÅ‡∏•‡∏∞ value
                                    # ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° alias ‡∏à‡∏≤‡∏Å symbol (‡πÄ‡∏ä‡πà‡∏ô "JPM80" -> "JPM")
                                    ticker_mapping[u_code] = u_code
                                    
                                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° alias ‡∏à‡∏≤‡∏Å symbol (‡∏ñ‡πâ‡∏≤ symbol ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà underlying code)
                                    sym_clean = item.get("symbol", "").strip().upper()
                                    if sym_clean and sym_clean != u_code:
                                        # ‡∏•‡∏ö suffix "80", "19" ‡∏à‡∏≤‡∏Å symbol
                                        sym_no_suffix = sym_clean.replace("80", "").replace("19", "").strip()
                                        if sym_no_suffix and sym_no_suffix != u_code and len(sym_no_suffix) <= 15 and ' ' not in sym_no_suffix:
                                            ticker_mapping[sym_no_suffix] = u_code
                                    
                                    # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° alias ‡∏à‡∏≤‡∏Å underlyingName (extract ticker ‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó)
                                    # ‡πÄ‡∏ä‡πà‡∏ô "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó JP MORGAN CHASE & CO. (JPM)" -> "JPM"
                                    if underlying_name:
                                        name_match = re.search(r'\(([A-Z0-9.\-_]+)\)', underlying_name.upper())
                                        if name_match:
                                            name_ticker = name_match.group(1).strip()
                                            if name_ticker and name_ticker != u_code and len(name_ticker) <= 15 and ' ' not in name_ticker:
                                                ticker_mapping[name_ticker] = u_code
                                    
                                    # Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£ extract (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 10 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
                                    if len(valid_dr_tickers) <= 10:
                                        print(f"    [OK] Extracted: {u_code} from {source} (symbol: {item.get('symbol', 'N/A')}, underlyingName: {underlying_name[:50]})")
                    
                    # Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å skip
                    if skipped_count > 0:
                        print(f"  [WARN] Skipped {skipped_count} items: {skipped_reasons}")
                        # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á items ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å skip (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 5 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
                        for skipped in skipped_items[:5]:
                            print(f"    - Skipped: symbol='{skipped['symbol']}', u_code='{skipped['u_code']}', reason={skipped['reason']}, underlyingName='{skipped['underlyingName']}'")
                    print(f"[Background] DR Filter is ENABLED. Found {len(valid_dr_tickers)} unique symbols (from {len(dr_rows)} DR rows, skipped {skipped_count}).")
                    # Debug: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ underlying codes ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    if len(valid_dr_tickers) < len(dr_rows) - skipped_count:
                        duplicate_count = len(dr_rows) - skipped_count - len(valid_dr_tickers)
                        print(f"  [NOTE] Note: {duplicate_count} underlying codes are duplicates (multiple DR rows share the same underlying code)")
                    # Debug: ‡πÅ‡∏™‡∏î‡∏á sample ‡∏Ç‡∏≠‡∏á underlying codes (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ticker symbols ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°)
                    # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ticker symbols ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ space ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    clean_tickers = [c for c in valid_dr_tickers if ' ' not in c and len(c) <= 15]
                    sample_codes = sorted(clean_tickers)[:10]
                    print(f"  [SAMPLE] Sample ticker symbols: {sample_codes}")
                    # Debug: ‡πÅ‡∏™‡∏î‡∏á ticker symbols ‡∏ó‡∏µ‡πà‡∏°‡∏µ space (‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö - ‡∏Ñ‡∏ß‡∏£‡πÑ‡∏°‡πà‡∏°‡∏µ
                    full_names = [c for c in valid_dr_tickers if ' ' in c]
                    if full_names:
                        print(f"  [WARN] Found {len(full_names)} full names (should be filtered out): {full_names[:5]}")
                    # Debug: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ "JPM", "WFC", "BAC", "MS" ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô valid_dr_tickers ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    test_tickers = ["JPM", "WFC", "BAC", "MS", "GS", "C"]
                    found_test = [t for t in test_tickers if t in valid_dr_tickers]
                    missing_test = [t for t in test_tickers if t not in valid_dr_tickers]
                    if found_test:
                        print(f"  [OK] Found test tickers in DR list: {found_test}")
                    if missing_test:
                        print(f"  [WARN] Missing test tickers in DR list: {missing_test}")
                    # Debug: ‡πÅ‡∏™‡∏î‡∏á sample ‡∏Ç‡∏≠‡∏á ticker_mapping
                    if ticker_mapping:
                        mapping_samples = list(ticker_mapping.items())[:10]
                        print(f"  [SAMPLE] Sample ticker mapping: {mapping_samples}")
                except Exception as dr_err:
                    print(f"[ERROR] [Background] Failed to fetch DR whitelist: {dr_err}")
                    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DR ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
                    valid_dr_tickers = None 
            else:
                print(f"[INFO] [Background] DR Filter is DISABLED. Fetching all stocks.")

            new_db = {}
            all_markets = ["america", "hongkong", "japan", "china", "singapore", "vietnam", "france", "netherlands", "denmark", "italy", "taiwan", "thailand"]
            
            for m in all_markets:
                c_code = "JP" if m == "japan" else "US"
                s_ts, e_ts = get_tradingview_range(c_code)
                print(f"[DATE] [Background] [{m}] Date range: {datetime.fromtimestamp(s_ts, tz=timezone.utc)} to {datetime.fromtimestamp(e_ts, tz=timezone.utc)}")
                
                raw_data = await fetch_tradingview_earnings(m, s_ts, e_ts)
                print(f"[DATA] [Background] [{m}] Received {len(raw_data)} raw items from TradingView")
                
                # Debug: ‡πÅ‡∏™‡∏î‡∏á sample ‡∏Ç‡∏≠‡∏á raw data structure (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 2 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
                if raw_data and len(raw_data) > 0:
                    sample = raw_data[0]
                    if isinstance(sample, dict) and "d" in sample:
                        d = sample.get("d", [])
                        if len(d) >= len(COLUMNS_MAP):
                            sample_obj = {COLUMNS_MAP[i]: d[i] for i in range(min(len(d), len(COLUMNS_MAP)))}
                            print(f"  [DEBUG] Sample TradingView item: logoid='{sample_obj.get('logoid')}', name='{sample_obj.get('name')}', description='{sample_obj.get('description', '')[:50]}'")
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏∏‡πâ‡∏ô 2653 ‡πÉ‡∏ô raw_data ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏•‡∏≤‡∏î‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô
                if m == "japan":
                    raw_2653 = [item for item in raw_data if item.get("d") and len(item.get("d", [])) > 1 and "2653" in str(item.get("d", [])[1]).upper()]
                    if raw_2653:
                        print(f"[OK] [Background] Found 2653 in raw_data: {raw_2653}")
                    else:
                        print(f"[WARN] [Background] 2653 NOT found in raw_data")
                
                # ‚úÖ ‡∏™‡πà‡∏á valid_dr_tickers ‡πÅ‡∏•‡∏∞ ticker_mapping ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô None ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏≠‡∏á)
                if valid_dr_tickers:
                    print(f"  [DEBUG] [Background] [{m}] Filtering with {len(valid_dr_tickers)} DR tickers. Sample: {list(valid_dr_tickers)[:5]}")
                    print(f"  [DEBUG] [Background] [{m}] Ticker mapping table has {len(ticker_mapping)} entries")
                stock_list = map_tv_data_to_object(raw_data, valid_dr_tickers, ticker_mapping)
                print(f"[OK] [Background] [{m}] Mapped to {len(stock_list)} stocks (from {len(raw_data)} raw items)")
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏∏‡πâ‡∏ô 2653 ‡πÉ‡∏ô stock_list ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏•‡∏≤‡∏î‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô
                if m == "japan":
                    ticker_2653 = [s for s in stock_list if "2653" in s.get("ticker", "")]
                    if ticker_2653:
                        print(f"[OK] [Background] Found 2653 in stock_list: {ticker_2653}")
                    else:
                        print(f"[WARN] [Background] 2653 NOT found in stock_list (was filtered out)")
                
                stock_list.sort(key=lambda x: x["date"] if x["date"] else float('inf'))
                
                display_name = MARKET_DISPLAY_NAMES.get(m, m.upper())
                if stock_list:
                    new_db[display_name] = {"totalCount": len(stock_list), "data": stock_list}
                await asyncio.sleep(0.5) 

            # Detect new earnings before updating
            new_earnings = find_new_earnings(new_db, _previous_earnings_db)
            
            # Update earnings database
            _earnings_db = new_db
            _last_update_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            save_db_to_disk()
            
            # Broadcast new earnings to SSE clients
            if new_earnings:
                print(f"[SSE] [Background] Found {len(new_earnings)} new earnings, broadcasting to SSE clients")
                await broadcast_to_sse_clients({
                    "type": "new_earnings",
                    "earnings": new_earnings,
                    "count": len(new_earnings),
                    "updated_at": _last_update_str
                })
            
            # Update previous earnings state for next comparison
            _previous_earnings_db = new_db.copy()
            
            print(f"[OK] [Background] Update complete. (ENABLE_DR_FILTER={ENABLE_DR_FILTER})")
        except Exception as e: print(f"[ERROR] Updater error: {e}")
        await asyncio.sleep(UPDATE_INTERVAL_SECONDS)

@asynccontextmanager
async def lifespan(app: FastAPI):
    load_db_from_disk()
    asyncio.create_task(background_updater())
    yield

app = FastAPI(lifespan=lifespan)

# DEV ONLY: Using wildcard CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
    allow_credentials=True,
)

@app.get("/api/earnings")
async def get_earnings(country: str = Query("US")):
    if country.upper() == "ALL":
        # Calculate date range for US market (default)
        s_ts, e_ts = get_tradingview_range("US")
        start_date = datetime.fromtimestamp(s_ts, tz=timezone.utc)
        end_date = datetime.fromtimestamp(e_ts, tz=timezone.utc)
        return {
            "updated_at": _last_update_str,
            "date_range": {
                "start": start_date.isoformat(),
                "end": end_date.isoformat(),
                "start_display": start_date.strftime("%Y-%m-%d %H:%M:%S UTC"),
                "end_display": end_date.strftime("%Y-%m-%d %H:%M:%S UTC"),
                "note": "Shows earnings from next Monday to next Sunday (excluding end date)"
            },
            "data": _earnings_db
        }
    
    market_code = get_market_code(country)
    display_name = MARKET_DISPLAY_NAMES.get(market_code, "")
    
    # Calculate date range for the requested country
    c_code = "JP" if market_code == "japan" else "US"
    s_ts, e_ts = get_tradingview_range(c_code)
    start_date = datetime.fromtimestamp(s_ts, tz=timezone.utc)
    end_date = datetime.fromtimestamp(e_ts, tz=timezone.utc)
    
    if display_name in _earnings_db:
        return {
            "updated_at": _last_update_str,
            "date_range": {
                "start": start_date.isoformat(),
                "end": end_date.isoformat(),
                "start_display": start_date.strftime("%Y-%m-%d %H:%M:%S UTC"),
                "end_display": end_date.strftime("%Y-%m-%d %H:%M:%S UTC"),
                "note": "Shows earnings from next Monday to next Sunday (excluding end date)"
            },
            "data": {display_name: _earnings_db[display_name]}
        }
    return {
        "updated_at": _last_update_str,
        "date_range": {
            "start": start_date.isoformat(),
            "end": end_date.isoformat(),
            "start_display": start_date.strftime("%Y-%m-%d %H:%M:%S UTC"),
            "end_display": end_date.strftime("%Y-%m-%d %H:%M:%S UTC"),
            "note": "Shows earnings from next Monday to next Sunday (excluding end date)"
        },
        "data": {}
    }

@app.get("/api/earnings/stream")
async def earnings_stream():
    """SSE endpoint for real-time earnings updates"""
    async def event_generator():
        # Create a queue for this client
        queue = asyncio.Queue()
        
        # Add client to the list
        async with _sse_lock:
            _sse_clients.append(queue)
            client_count = len(_sse_clients)
        
        print(f"[SSE] New client connected. Total clients: {client_count}")
        
        try:
            # Send initial connection message
            yield f"data: {json.dumps({'type': 'connected', 'message': 'SSE connection established'})}\n\n"
            
            # Send heartbeat every 30 seconds to keep connection alive
            last_heartbeat = time_module.time()
            heartbeat_interval = 30
            
            while True:
                # Wait for message with timeout for heartbeat
                current_time = time_module.time()
                time_since_heartbeat = current_time - last_heartbeat
                timeout = max(0.1, heartbeat_interval - time_since_heartbeat)
                
                try:
                    message = await asyncio.wait_for(queue.get(), timeout=timeout)
                    yield f"data: {json.dumps(message)}\n\n"
                except asyncio.TimeoutError:
                    # Send heartbeat (uncomment to see heartbeats in console)
                    # print(f"üíì [SSE] Sending heartbeat to {len(_sse_clients)} clients")
                    yield f"data: {json.dumps({'type': 'heartbeat', 'timestamp': datetime.now().isoformat()})}\n\n"
                    last_heartbeat = time_module.time()
                        
        except asyncio.CancelledError:
            # Client disconnected
            print(f"[SSE] Client disconnected")
        finally:
            # Remove client from the list
            async with _sse_lock:
                if queue in _sse_clients:
                    _sse_clients.remove(queue)
                    remaining = len(_sse_clients)
                    print(f"[SSE] Client removed. Remaining clients: {remaining}")
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

@app.post("/api/earnings/refresh")
async def force_refresh_earnings():
    """Force refresh earnings data immediately (bypass cache interval)"""
    global _earnings_db, _last_update_str, _previous_earnings_db
    try:
        print(f"[REFRESH] [Manual Refresh] Forcing earnings update at {datetime.now()}")
        
        # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Logic ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ DR ‡∏ï‡∏≤‡∏°‡∏Ñ‡πà‡∏≤ ENABLE_DR_FILTER
        valid_dr_tickers = None
        ticker_mapping = {}  # Mapping table: {ticker_from_tv: underlying_code}
        if ENABLE_DR_FILTER:
            valid_dr_tickers = set()
            skipped_count = 0
            skipped_reasons = {}
            skipped_items = []
            try:
                async with httpx.AsyncClient() as client:
                    r_dr = await client.get(DR_LIST_URL, timeout=10)
                    dr_rows = r_dr.json().get("rows", [])
                    print(f"  [DATA] [Manual Refresh] Total DR rows from API: {len(dr_rows)}")
                    for item in dr_rows:
                        u_code = None
                        source = None
                        
                        underlying_name = item.get("underlyingName") or ""
                        match = re.search(r'\(([A-Z0-9.\-_]+)\)$', underlying_name)
                        if match:
                            u_code = match.group(1)
                            source = "underlyingName"
                        else:
                            u_code = item.get("underlying")
                            if u_code:
                                source = "underlying"
                            else:
                                sym = item.get("symbol") or ""
                                if "80" in sym: 
                                    u_code = sym.replace("80", "")
                                    source = "symbol(80)"
                                elif "19" in sym: 
                                    u_code = sym.replace("19", "")
                                    source = "symbol(19)"
                        
                        if u_code:
                            u_code = u_code.strip().upper()
                            if u_code and len(u_code) > 0:
                                if ' ' in u_code:
                                    name_match_alt = re.search(r'\(([A-Z0-9.\-_]+)\)', underlying_name.upper())
                                    if name_match_alt:
                                        alt_ticker = name_match_alt.group(1).strip()
                                        if alt_ticker and ' ' not in alt_ticker and len(alt_ticker) <= 15:
                                            u_code = alt_ticker
                                            source = "underlyingName(alt)"
                                        else:
                                            u_code_clean = re.sub(r'\s+(ETF|DIAMOND ETF|FUND|TRUST).*$', '', u_code, flags=re.IGNORECASE).strip()
                                            if u_code_clean and ' ' not in u_code_clean and len(u_code_clean) <= 15:
                                                u_code = u_code_clean
                                                source = "underlying(clean)"
                                            else:
                                                sym = item.get("symbol") or ""
                                                if sym:
                                                    sym_clean = re.sub(r'\d+$', '', sym).strip()
                                                    if sym_clean and len(sym_clean) >= 2 and len(sym_clean) <= 15:
                                                        u_code = sym_clean.upper()
                                                        source = "symbol(clean)"
                                                    else:
                                                        skipped_count += 1
                                                        skipped_reasons['has_space'] = skipped_reasons.get('has_space', 0) + 1
                                                        skipped_items.append({
                                                            'symbol': item.get('symbol', 'N/A'),
                                                            'underlyingName': underlying_name[:50],
                                                            'u_code': u_code,
                                                            'reason': 'has_space'
                                                        })
                                                        continue
                                                else:
                                                    skipped_count += 1
                                                    skipped_reasons['has_space'] = skipped_reasons.get('has_space', 0) + 1
                                                    skipped_items.append({
                                                        'symbol': item.get('symbol', 'N/A'),
                                                        'underlyingName': underlying_name[:50],
                                                        'u_code': u_code,
                                                        'reason': 'has_space'
                                                    })
                                                    continue
                                    else:
                                        u_code_clean = re.sub(r'\s+(ETF|DIAMOND ETF|FUND|TRUST).*$', '', u_code, flags=re.IGNORECASE).strip()
                                        if u_code_clean and ' ' not in u_code_clean and len(u_code_clean) <= 15:
                                            u_code = u_code_clean
                                            source = "underlying(clean)"
                                        else:
                                            sym = item.get("symbol") or ""
                                            if sym:
                                                sym_clean = re.sub(r'\d+$', '', sym).strip()
                                                if sym_clean and len(sym_clean) >= 2 and len(sym_clean) <= 15:
                                                    u_code = sym_clean.upper()
                                                    source = "symbol(clean)"
                                                else:
                                                    skipped_count += 1
                                                    skipped_reasons['has_space'] = skipped_reasons.get('has_space', 0) + 1
                                                    skipped_items.append({
                                                        'symbol': item.get('symbol', 'N/A'),
                                                        'underlyingName': underlying_name[:50],
                                                        'u_code': u_code,
                                                        'reason': 'has_space'
                                                    })
                                                    continue
                                            else:
                                                skipped_count += 1
                                                skipped_reasons['has_space'] = skipped_reasons.get('has_space', 0) + 1
                                                skipped_items.append({
                                                    'symbol': item.get('symbol', 'N/A'),
                                                    'underlyingName': underlying_name[:50],
                                                    'u_code': u_code,
                                                    'reason': 'has_space'
                                                })
                                                continue
                                if len(u_code) > 15:
                                    skipped_count += 1
                                    skipped_reasons['too_long'] = skipped_reasons.get('too_long', 0) + 1
                                    skipped_items.append({
                                        'symbol': item.get('symbol', 'N/A'),
                                        'underlyingName': underlying_name[:50],
                                        'u_code': u_code,
                                        'reason': 'too_long'
                                    })
                                    continue
                                valid_dr_tickers.add(u_code)
                                ticker_mapping[u_code] = u_code
                                
                                sym_clean = item.get("symbol", "").strip().upper()
                                if sym_clean and sym_clean != u_code:
                                    sym_no_suffix = sym_clean.replace("80", "").replace("19", "").strip()
                                    if sym_no_suffix and sym_no_suffix != u_code and len(sym_no_suffix) <= 15 and ' ' not in sym_no_suffix:
                                        ticker_mapping[sym_no_suffix] = u_code
                                
                                if underlying_name:
                                    name_match = re.search(r'\(([A-Z0-9.\-_]+)\)', underlying_name.upper())
                                    if name_match:
                                        name_ticker = name_match.group(1).strip()
                                        if name_ticker and name_ticker != u_code and len(name_ticker) <= 15 and ' ' not in name_ticker:
                                            ticker_mapping[name_ticker] = u_code
                    
                    if skipped_count > 0:
                        print(f"  [WARN] Skipped {skipped_count} items: {skipped_reasons}")
                    print(f"[DATA] [Manual Refresh] DR Filter is ENABLED. Found {len(valid_dr_tickers)} unique symbols.")
            except Exception as dr_err:
                print(f"[ERROR] [Manual Refresh] Failed to fetch DR whitelist: {dr_err}")
                valid_dr_tickers = None 
        else:
            print(f"[INFO] [Manual Refresh] DR Filter is DISABLED. Fetching all stocks.")

        new_db = {}
        all_markets = ["america", "hongkong", "japan", "china", "singapore", "vietnam", "france", "netherlands", "denmark", "italy", "taiwan", "thailand"]
        
        for m in all_markets:
            c_code = "JP" if m == "japan" else "US"
            s_ts, e_ts = get_tradingview_range(c_code)
            print(f"[DATE] [Manual Refresh] [{m}] Date range: {datetime.fromtimestamp(s_ts, tz=timezone.utc)} to {datetime.fromtimestamp(e_ts, tz=timezone.utc)}")
            
            raw_data = await fetch_tradingview_earnings(m, s_ts, e_ts)
            print(f"[DATA] [Manual Refresh] [{m}] Received {len(raw_data)} raw items from TradingView")
            
            stock_list = map_tv_data_to_object(raw_data, valid_dr_tickers, ticker_mapping)
            print(f"[OK] [Manual Refresh] [{m}] Mapped to {len(stock_list)} stocks")
            
            stock_list.sort(key=lambda x: x["date"] if x["date"] else float('inf'))
            
            display_name = MARKET_DISPLAY_NAMES.get(m, m.upper())
            if stock_list:
                new_db[display_name] = {"totalCount": len(stock_list), "data": stock_list}
            await asyncio.sleep(0.5)

        # Detect new earnings before updating
        new_earnings = find_new_earnings(new_db, _previous_earnings_db)
        
        # Update earnings database
        _earnings_db = new_db
        _last_update_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        save_db_to_disk()
        
        # Broadcast new earnings to SSE clients
        if new_earnings:
            print(f"[SSE] [Manual Refresh] Found {len(new_earnings)} new earnings, broadcasting to SSE clients")
            await broadcast_to_sse_clients({
                "type": "new_earnings",
                "earnings": new_earnings,
                "count": len(new_earnings),
                "updated_at": _last_update_str
            })
        
        # Update previous earnings state for next comparison
        _previous_earnings_db = new_db.copy()
        
        return {
            "success": True,
            "message": "Earnings data refreshed successfully",
            "updated_at": _last_update_str,
            "markets": list(new_db.keys()),
            "total_earnings": sum(m.get("totalCount", 0) for m in new_db.values()),
            "new_earnings_count": len(new_earnings)
        }
    except Exception as e:
        print(f"[ERROR] [Manual Refresh] Error: {e}")
        return {"success": False, "error": str(e)}

@app.get("/api/test")
async def get_earnings(country: str = Query("US")):
    return {'d': 1234}

if __name__ == "__main__":
    uvicorn.run("earnings_api:app", host="0.0.0.0", port=3001, reload=True)
